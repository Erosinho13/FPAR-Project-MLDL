{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:58:30.039047Z",
     "start_time": "2020-05-28T16:58:29.287662Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gtea_dataset import GTEA61, GTEA61_flow, GTEA61_2Stream\n",
    "from objectAttentionModelConvLSTM import attentionModel\n",
    "from flow_resnet import flow_resnet34\n",
    "from twoStreamModel import twoStreamAttentionModel\n",
    "from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
    "                                RandomHorizontalFlip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:58:31.468217Z",
     "start_time": "2020-05-28T16:58:31.462671Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Logger():\n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "        self.data = []\n",
    "        self.step_data = []\n",
    "        \n",
    "    def add_epoch_data(self, epoch, acc, loss):\n",
    "        self.data.append({epoch:(acc, loss)})\n",
    "        \n",
    "    def add_step_data(self, step, acc, loss):\n",
    "        self.step_data.append({step:(acc, loss)})\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as logfile:\n",
    "            pickle.dump(self, logfile)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(path, 'rb') as logfile:\n",
    "            new_instance = pickle.load(logfile)\n",
    "        return new_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:41:50.388172Z",
     "start_time": "2020-05-26T20:41:50.380551Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 0.001            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 200      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [25, 75, 150] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "SEQ_LEN = 7\n",
    "\n",
    "model_folder = 'saved_models'\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_EPOCHS, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'SEQ_LEN':SEQ_LEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:41:51.393477Z",
     "start_time": "2020-05-26T20:41:51.390053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
    "                             ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:41:52.228708Z",
     "start_time": "2020-05-26T20:41:52.190562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/home/utente/Scaricati/program/ML_DL/FPAR/GTEA61'\n",
    "\n",
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n",
    "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform, seq_len=SEQ_LEN)\n",
    "\n",
    "#train_indexes, val_indexes  = train_dataset.split_indices(0.6)\n",
    "\n",
    "#val_dataset = Subset(train_dataset, val_indexes)\n",
    "#train_dataset = Subset(train_dataset, train_indexes)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "#print('Valid Dataset: {}'.format(len(val_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:41:53.219370Z",
     "start_time": "2020-05-26T20:41:53.214299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "#val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:21:43.774688Z",
     "start_time": "2020-05-26T20:21:40.711267Z"
    }
   },
   "outputs": [],
   "source": [
    "validate = True\n",
    "\n",
    "model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n",
    "model.train(False)\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "for params in model.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "model.lstm_cell.train(True)\n",
    "model.classifier.train(True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:02:41.949191Z",
     "start_time": "2020-05-23T12:02:41.946130Z"
    }
   },
   "source": [
    "# Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:14:42.350120Z",
     "start_time": "2020-05-26T19:13:14.471232Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log_file = os.path.join(model_folder, \"log_stage1.obj\")\n",
    "val_log_file = os.path.join(model_folder, \"val_log_stage1.obj\")\n",
    "train_logger = Logger(**parameters)\n",
    "val_logger = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    model.lstm_cell.train(True)\n",
    "    model.classifier.train(True)\n",
    "        \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        output_label, _ = model(inputVariable)\n",
    "        \n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "            \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "    print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "    if validate:\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "                \n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "            val_logger.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "\n",
    "            print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val||||]\")\n",
    "                save_path_model = os.path.join(model_folder, 'model_rgb_state_dict.pth')\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "        \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        save_path_model = os.path.join(model_folder, '/model_rgb_state_dict_epoch' + str(epoch+1) + '.pth')\n",
    "        torch.save(model.state_dict(), save_path_model)\n",
    "    train_logger.save(train_log_file)\n",
    "    val_logger.save(val_log_file)\n",
    "    optim_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 0.0001            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 150      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [25, 75] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "SEQ_LEN = 7\n",
    "\n",
    "model_folder = 'saved_models'\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_EPOCHS, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'SEQ_LEN':SEQ_LEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_dict = os.path.join(model_folder, 'model_rgb_state_dict.pth')\n",
    "validate = True\n",
    "\n",
    "model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n",
    "model.load_state_dict(torch.load(stage1_dict))\n",
    "model.train(False)\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "        \n",
    "for params in model.resNet.layer4[0].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "for params in model.resNet.layer4[0].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "for params in model.resNet.layer4[1].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "for params in model.resNet.layer4[1].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "for params in model.resNet.layer4[2].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "#\n",
    "for params in model.resNet.layer4[2].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "#\n",
    "for params in model.resNet.fc.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "model.resNet.layer4[0].conv1.train(True)\n",
    "model.resNet.layer4[0].conv2.train(True)\n",
    "model.resNet.layer4[1].conv1.train(True)\n",
    "model.resNet.layer4[1].conv2.train(True)\n",
    "model.resNet.layer4[2].conv1.train(True)\n",
    "model.resNet.layer4[2].conv2.train(True)\n",
    "model.resNet.fc.train(True)\n",
    "\n",
    "for params in model.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "\n",
    "model.lstm_cell.train(True)\n",
    "model.classifier.train(True)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer_fn = torch.optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
    "\n",
    "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log_file = os.path.join(model_folder, \"log_stage2.obj\")\n",
    "val_log_file = os.path.join(model_folder, \"val_log_stage2.obj\")\n",
    "train_logger_2 = Logger(**parameters)\n",
    "val_logger_2 = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    \n",
    "    model.lstm_cell.train(True)\n",
    "    model.classifier.train(True)\n",
    "    model.resNet.layer4[0].conv1.train(True)\n",
    "    model.resNet.layer4[0].conv2.train(True)\n",
    "    model.resNet.layer4[1].conv1.train(True)\n",
    "    model.resNet.layer4[1].conv2.train(True)\n",
    "    model.resNet.layer4[2].conv1.train(True)\n",
    "    model.resNet.layer4[2].conv2.train(True)\n",
    "    model.resNet.fc.train(True)\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        output_label, _ = model(inputVariable)\n",
    "        \n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger_2.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "\n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger_2.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "    \n",
    "    print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "    if validate is not None:\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            val_samples = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger_2.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "\n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "            print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            val_logger_2.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "            \n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val |||]\")\n",
    "                save_path_model = (model_folder + '/model_rgb_state_dict_stage2.pth')\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            \n",
    "    if (epoch+1) % 40 == 0:\n",
    "        save_path_model = (model_folder + '/model_rgb_state_dict_stage2_epoch' + str(epoch+1) + '.pth')\n",
    "        torch.save(model.state_dict(), save_path_model)\n",
    "    optim_scheduler.step()\n",
    "    train_logger_2.save(train_log_file)\n",
    "    val_logger_2.save(val_log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train temporal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:58:54.683860Z",
     "start_time": "2020-05-28T16:58:54.676306Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 0.01            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-4  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 750      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [150, 300, 500] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.5          # Multiplicative factor for learning rate step-down\n",
    "STACK_SIZE = 5\n",
    "\n",
    "model_folder = 'saved_models'\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_EPOCHS, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'STACK_SIZE':STACK_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:58:58.427204Z",
     "start_time": "2020-05-28T16:58:58.421684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "spatial_transform_train = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
    "                             ToTensor(), normalize])\n",
    "spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:59:01.228238Z",
     "start_time": "2020-05-28T16:59:00.713373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/home/utente/Scaricati/program/ML_DL/FPAR/GTEA61'\n",
    "\n",
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61_flow(DATA_DIR, split='train', transform=spatial_transform_train, seq_len=STACK_SIZE)\n",
    "test_dataset = GTEA61_flow(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=STACK_SIZE)\n",
    "\n",
    "#train_indexes, val_indexes  = train_dataset.split_indices(0.6)\n",
    "\n",
    "#val_dataset = Subset(train_dataset, val_indexes)\n",
    "#train_dataset = Subset(train_dataset, train_indexes)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "#print('Valid Dataset: {}'.format(len(val_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:59:03.403407Z",
     "start_time": "2020-05-28T16:59:03.398494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:59:10.486808Z",
     "start_time": "2020-05-28T16:59:06.518557Z"
    }
   },
   "outputs": [],
   "source": [
    "validate = True\n",
    "\n",
    "model = flow_resnet34(True, channels=2*STACK_SIZE, num_classes=NUM_CLASSES)\n",
    "model.train(True)\n",
    "train_params = list(model.parameters())\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = torch.optim.SGD(train_params, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:01.771262Z",
     "start_time": "2020-05-24T15:16:44.278334Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log_file = os.path.join(model_folder, \"log_stage3.obj\")\n",
    "val_log_file = os.path.join(model_folder, \"val_log_stage3.obj\")\n",
    "train_logger_3 = Logger(**parameters)\n",
    "val_logger_3 = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    model.train(True)\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        \n",
    "        output_label, _ = model(inputVariable)\n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger_3.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch + 1, avg_loss, trainAccuracy))\n",
    "    train_logger_3.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "        \n",
    "    if validate:\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            val_samples = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                val_samples += inputs.size(0)\n",
    "                inputVariable = inputs.to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                step_loss = val_loss.data.item()\n",
    "                val_loss_epoch += step_loss\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger_3.add_step_data(val_iter, numCorr, step_loss)\n",
    "                \n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "            val_logger_3.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "            \n",
    "            print('Validation: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                save_path_model = (model_folder + '/model_flow_state_dict.pth')\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        save_path_model = (model_folder + '/model_flow_state_dict_epoch' + str(epoch+1) + '.pth')\n",
    "        torch.save(model.state_dict(), save_path_model)\n",
    "    optim_scheduler.step()\n",
    "    train_logger_3.save(train_log_file)\n",
    "    val_logger_3.save(val_log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:20:13.394143Z",
     "start_time": "2020-05-24T13:20:13.390030Z"
    }
   },
   "source": [
    "# 2 Stream joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:22:59.772114Z",
     "start_time": "2020-05-27T17:22:59.766728Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 0.01            # The initial Learning Rate\n",
    "LR_FLOW = 0.0001\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-4  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 250      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = 1 # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.99          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "STACK_SIZE = 5\n",
    "SEQ_LEN = 7\n",
    "\n",
    "LOG_FREQUENCY = 20\n",
    "\n",
    "model_folder = '../saved_models'\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_EPOCHS, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'SEQ_LEN':STACK_SIZE, 'STACK_SIZE':STACK_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:23:00.616614Z",
     "start_time": "2020-05-27T17:23:00.611324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "spatial_transform_train = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
    "                             ToTensor(), normalize])\n",
    "spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:23:01.475430Z",
     "start_time": "2020-05-27T17:23:01.203650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/home/utente/Scaricati/program/ML_DL/FPAR/GTEA61'\n",
    "\n",
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61_2Stream(DATA_DIR, split='train', transform=spatial_transform_train, seq_len=SEQ_LEN, \n",
    "                               stack_size=STACK_SIZE)\n",
    "test_dataset = GTEA61_2Stream(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN, \n",
    "                              stack_size=STACK_SIZE)\n",
    "\n",
    "#train_indexes, val_indexes  = train_dataset.split_indices(0.6)\n",
    "\n",
    "#val_dataset = Subset(train_dataset, val_indexes)\n",
    "#train_dataset = Subset(train_dataset, train_indexes)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "#print('Valid Dataset: {}'.format(len(val_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:23:02.025071Z",
     "start_time": "2020-05-27T17:23:02.021724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:30:15.034943Z",
     "start_time": "2020-05-27T17:30:09.603289Z"
    }
   },
   "outputs": [],
   "source": [
    "flowModel = os.path.join(model_folder, \"model_flow_state_dict.pth\")\n",
    "rgbModel = os.path.join(model_folder, \"model_rgb_state_dict_stage2.pth\")\n",
    "validate = True\n",
    "\n",
    "model = twoStreamAttentionModel(flowModel=flowModel, frameModel=rgbModel, stackSize=STACK_SIZE, memSize=MEM_SIZE,\n",
    "                                    num_classes=NUM_CLASSES)\n",
    "\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "model.train(False)\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.resNet.layer4[0].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.resNet.layer4[0].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "       \n",
    "for params in model.frameModel.resNet.layer4[1].conv1.parameters():\n",
    "    params.requires_grad = True     \n",
    "\n",
    "for params in model.frameModel.resNet.layer4[1].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "       \n",
    "for params in model.frameModel.resNet.layer4[2].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.resNet.layer4[2].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "            \n",
    "for params in model.frameModel.resNet.fc.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.flowModel.layer4.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "\n",
    "model.classifier.train(True)\n",
    "model.flowModel.layer4.train(True)\n",
    "model.frameModel.lstm_cell.train(True)\n",
    "model.frameModel.classifier.train(True)\n",
    "\n",
    "frame_trainable_params = [p for p in model.frameModel.parameters() if p.requires_grad]\n",
    "flow_trainable_params = [p for p in model.flowModel.parameters() if p.requires_grad]\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = torch.optim.SGD([\n",
    "        {'params': frame_trainable_params},\n",
    "        {'params': flow_trainable_params, 'lr': LR_FLOW},\n",
    "    ], lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.StepLR(optimizer_fn, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:36:26.054236Z",
     "start_time": "2020-05-27T17:32:48.646035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss after 1 epoch = 4.344 \n",
      "Training accuracy after 1.000 epoch = 2.8125% \n",
      "Average training loss after 2 epoch = 4.262 \n",
      "Training accuracy after 2.000 epoch = 1.5625% \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valSamples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ba6f4dbc4e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mval_logger_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_step_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumCorr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumCorr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvalSamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mavg_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss_epoch\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mval_logger_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_epoch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valSamples' is not defined"
     ]
    }
   ],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log_file = os.path.join(model_folder, \"log_stage4.obj\")\n",
    "val_log_file = os.path.join(model_folder, \"val_log_stage4.obj\")\n",
    "train_logger_4 = Logger(**parameters)\n",
    "val_logger_4 = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    model.classifier.train(True)\n",
    "    model.flowModel.layer4.train(True)\n",
    "    model.frameModel.lstm_cell.train(True)\n",
    "    model.frameModel.classifier.train(True)\n",
    "    for j, (inputFlow, inputFrame, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        \n",
    "        inputVariableFlow = inputFlow.to(DEVICE)\n",
    "        inputVariableFrame = inputFrame.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        \n",
    "        output_label = model(inputVariableFlow, inputVariableFrame)\n",
    "        loss = loss_fn(F.log_softmax(output_label, dim=1), labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger_4.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "\n",
    "    avg_loss = epoch_loss / iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger_4.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "\n",
    "    print('Average training loss after {} epoch = {:.3f} '.format(epoch + 1, avg_loss))\n",
    "    print('Training accuracy after {} epoch = {:.3f}% '.format(epoch + 1, trainAccuracy))\n",
    "    \n",
    "    if validate:\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputFlow, inputFrame, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                \n",
    "                inputVariableFlow = inputFlow.to(DEVICE)\n",
    "                inputVariableFrame = inputFrame.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                 \n",
    "                output_label = model(inputVariableFlow, inputVariableFrame)\n",
    "                val_loss = loss_fn(F.log_softmax(output_label, dim=1), labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger_4.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "\n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_iter\n",
    "            val_logger_4.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "\n",
    "            print('Val Loss after {} epochs, loss = {:.3f}'.format(epoch + 1, avg_val_loss))\n",
    "            print('Val Accuracy after {:.3f} epochs = {}%'.format(epoch + 1, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val |||]\")\n",
    "                save_path_model = os.path.join(model_folder, 'model_twoStream_state_dict.pth')\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        save_path_model = os.path.join(model_folder,'/model_twoStream_state_dict_epoch' + str(epoch + 1) + '.pth')\n",
    "        torch.save(model.state_dict(), save_path_model)\n",
    "    optim_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
