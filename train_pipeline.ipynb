{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:25:15.118952Z",
     "start_time": "2020-05-24T15:25:15.113529Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gtea_dataset import GTEA61, GTEA61_flow, GTEA61_2Stream\n",
    "from objectAttentionModelConvLSTM import attentionModel\n",
    "from flow_resnet import flow_resnet34\n",
    "from twoStreamModel import twoStreamAttentionModel\n",
    "from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
    "                                RandomHorizontalFlip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:40:41.186518Z",
     "start_time": "2020-05-24T14:40:41.181391Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 0.001            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 300      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [25, 75, 150] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "STACK_SIZE = 5\n",
    "SEQ_LEN = 7\n",
    "\n",
    "LOG_FREQUENCY = 20\n",
    "\n",
    "model_folder = 'saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:03:07.900929Z",
     "start_time": "2020-05-23T12:03:07.895722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
    "                             ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:03:11.077294Z",
     "start_time": "2020-05-23T12:03:11.033820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/home/utente/Scaricati/program/ML_DL/FPAR/GTEA61'\n",
    "\n",
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n",
    "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform, seq_len=SEQ_LEN)\n",
    "\n",
    "#train_indexes, val_indexes  = train_dataset.split_indices(0.6)\n",
    "\n",
    "#val_dataset = Subset(train_dataset, val_indexes)\n",
    "#train_dataset = Subset(train_dataset, train_indexes)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "#print('Valid Dataset: {}'.format(len(val_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:03:15.024406Z",
     "start_time": "2020-05-23T12:03:15.018717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "#val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T11:45:01.015066Z",
     "start_time": "2020-05-23T11:44:55.200068Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "min_accuracy = 0\n",
    "epoch_loss = 0\n",
    "numCorrTrain = 0\n",
    "trainSamples = 0\n",
    "trainSamples_2 = 0\n",
    "iterPerEpoch = 0\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    train_iter += 1\n",
    "    iterPerEpoch += 1\n",
    "    trainSamples_2 += inputs.size(0)\n",
    "    inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "    labelVariable = targets.to(DEVICE)\n",
    "    trainSamples += inputs.size(0)\n",
    "    print(trainSamples, trainSamples_2, inputs.shape, inputVariable.shape, len(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:03:21.443291Z",
     "start_time": "2020-05-23T12:03:18.231573Z"
    }
   },
   "outputs": [],
   "source": [
    "validate = True\n",
    "\n",
    "model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n",
    "model.train(False)\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "for params in model.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "model.lstm_cell.train(True)\n",
    "model.classifier.train(True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:02:41.949191Z",
     "start_time": "2020-05-23T12:02:41.946130Z"
    }
   },
   "source": [
    "# Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:39:16.437977Z",
     "start_time": "2020-05-23T12:03:24.170695Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch = 1 | Loss = 4.116372776031494 | Accuracy = 4.375\n",
      "Train: Epoch = 2 | Loss = 4.030084347724914 | Accuracy = 5.0\n",
      "Val: Epoch = 2 | Loss 3.9895098209381104 | Accuracy = 4.310344827586207\n",
      "Train: Epoch = 3 | Loss = 3.971208190917969 | Accuracy = 6.25\n",
      "Train: Epoch = 4 | Loss = 3.905468201637268 | Accuracy = 7.5\n",
      "Val: Epoch = 4 | Loss 3.9058555364608765 | Accuracy = 11.206896551724139\n",
      "Train: Epoch = 5 | Loss = 3.767634916305542 | Accuracy = 8.75\n",
      "Train: Epoch = 6 | Loss = 3.690426754951477 | Accuracy = 10.625\n",
      "Val: Epoch = 6 | Loss 3.698900878429413 | Accuracy = 8.620689655172415\n",
      "Train: Epoch = 7 | Loss = 3.659843897819519 | Accuracy = 13.125\n",
      "Train: Epoch = 8 | Loss = 3.544969987869263 | Accuracy = 12.1875\n",
      "Val: Epoch = 8 | Loss 3.6253132820129395 | Accuracy = 17.24137931034483\n",
      "Train: Epoch = 9 | Loss = 3.5026852607727053 | Accuracy = 13.125\n",
      "Train: Epoch = 10 | Loss = 3.4243183851242067 | Accuracy = 13.750000000000002\n",
      "Val: Epoch = 10 | Loss 3.686962604522705 | Accuracy = 6.896551724137931\n",
      "Train: Epoch = 11 | Loss = 3.422571063041687 | Accuracy = 15.312500000000002\n",
      "Train: Epoch = 12 | Loss = 3.2725234508514403 | Accuracy = 15.312500000000002\n",
      "Val: Epoch = 12 | Loss 3.606597363948822 | Accuracy = 12.931034482758621\n",
      "Train: Epoch = 13 | Loss = 3.2515469074249266 | Accuracy = 16.25\n",
      "Train: Epoch = 14 | Loss = 3.267044281959534 | Accuracy = 20.0\n",
      "Val: Epoch = 14 | Loss 3.539809763431549 | Accuracy = 8.620689655172415\n",
      "Train: Epoch = 15 | Loss = 3.215160775184631 | Accuracy = 15.937499999999998\n",
      "Train: Epoch = 16 | Loss = 3.153627038002014 | Accuracy = 18.75\n",
      "Val: Epoch = 16 | Loss 3.306169807910919 | Accuracy = 18.96551724137931\n",
      "Train: Epoch = 17 | Loss = 3.0560722589492797 | Accuracy = 20.3125\n",
      "Train: Epoch = 18 | Loss = 3.0026498556137087 | Accuracy = 19.6875\n",
      "Val: Epoch = 18 | Loss 3.4625880122184753 | Accuracy = 13.793103448275861\n",
      "Train: Epoch = 19 | Loss = 2.9149447441101075 | Accuracy = 20.3125\n",
      "Train: Epoch = 20 | Loss = 2.917124629020691 | Accuracy = 20.0\n",
      "Val: Epoch = 20 | Loss 3.2279459834098816 | Accuracy = 23.275862068965516\n",
      "Train: Epoch = 21 | Loss = 2.853814196586609 | Accuracy = 23.75\n",
      "Train: Epoch = 22 | Loss = 2.8425429344177244 | Accuracy = 22.1875\n",
      "Val: Epoch = 22 | Loss 3.2948328852653503 | Accuracy = 18.103448275862068\n",
      "Train: Epoch = 23 | Loss = 2.7628414392471314 | Accuracy = 25.937500000000004\n",
      "Train: Epoch = 24 | Loss = 2.6958220481872557 | Accuracy = 25.937500000000004\n",
      "Val: Epoch = 24 | Loss 3.266000986099243 | Accuracy = 18.96551724137931\n",
      "Train: Epoch = 25 | Loss = 2.740789198875427 | Accuracy = 27.8125\n",
      "Train: Epoch = 26 | Loss = 2.6420475006103517 | Accuracy = 29.375\n",
      "Val: Epoch = 26 | Loss 3.2143828868865967 | Accuracy = 24.137931034482758\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c71b9ea9eb73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnumCorrTrain\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabelVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iter = 0\n",
    "min_accuracy = 0\n",
    "cudnn.benchmark\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    trainSamples = 0\n",
    "    iterPerEpoch = 0\n",
    "    model.lstm_cell.train(True)\n",
    "    model.classifier.train(True)\n",
    "        \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        iterPerEpoch += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        trainSamples += inputs.size(0)\n",
    "        output_label, _ = model(inputVariable)\n",
    "        \n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "            \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        epoch_loss += loss.data.item()\n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "\n",
    "    print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "    if validate:\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            val_iter = 0\n",
    "            val_samples = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                val_samples += inputs.size(0)\n",
    "                \n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_epoch += val_loss.data.item()\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                    \n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_iter\n",
    "            print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                save_path_model = (model_folder + '/model_rgb_state_dict.pth')\n",
    "                #torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            else:\n",
    "                if (epoch+1) % 10 == 0:\n",
    "                    save_path_model = (model_folder + '/model_rgb_state_dict_epoch' + str(epoch+1) + '.pth')\n",
    "                    #torch.save(model.state_dict(), save_path_model)\n",
    "    \n",
    "    optim_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_dict = \"best_model_dict.pth\"\n",
    "validate = True\n",
    "\n",
    "model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n",
    "model.load_state_dict(torch.load(stage1_dict))\n",
    "model.train(False)\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "        \n",
    "for params in model.resNet.layer4[0].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "for params in model.resNet.layer4[0].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "for params in model.resNet.layer4[1].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "for params in model.resNet.layer4[1].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "for params in model.resNet.layer4[2].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "#\n",
    "for params in model.resNet.layer4[2].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "#\n",
    "for params in model.resNet.fc.parameters():\n",
    "    params.requires_grad = True\n",
    "    train_params += [params]\n",
    "\n",
    "model.resNet.layer4[0].conv1.train(True)\n",
    "model.resNet.layer4[0].conv2.train(True)\n",
    "model.resNet.layer4[1].conv1.train(True)\n",
    "model.resNet.layer4[1].conv2.train(True)\n",
    "model.resNet.layer4[2].conv1.train(True)\n",
    "model.resNet.layer4[2].conv2.train(True)\n",
    "model.resNet.fc.train(True)\n",
    "\n",
    "for params in model.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "\n",
    "model.lstm_cell.train(True)\n",
    "model.classifier.train(True)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer_fn = torch.optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
    "\n",
    "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "min_accuracy = 0\n",
    "cudnn.benchmark\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    trainSamples = 0\n",
    "    iterPerEpoch = 0\n",
    "    \n",
    "    model.lstm_cell.train(True)\n",
    "    model.classifier.train(True)\n",
    "    model.resNet.layer4[0].conv1.train(True)\n",
    "    model.resNet.layer4[0].conv2.train(True)\n",
    "    model.resNet.layer4[1].conv1.train(True)\n",
    "    model.resNet.layer4[1].conv2.train(True)\n",
    "    model.resNet.layer4[2].conv1.train(True)\n",
    "    model.resNet.layer4[2].conv2.train(True)\n",
    "    model.resNet.fc.train(True)\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        iterPerEpoch += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        trainSamples += inputs.size(0)\n",
    "        output_label, _ = model(inputVariable)\n",
    "        \n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        epoch_loss += loss.data.item()\n",
    "            \n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "\n",
    "    print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "    if validate is not None:\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            val_iter = 0\n",
    "            val_samples = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                val_samples += inputs.size(0)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_epoch += val_loss.data.item()\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                \n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_iter\n",
    "            print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            \n",
    "            if val_accuracy > min_accuracy:\n",
    "                save_path_model = (model_folder + '/model_rgb_state_dict.pth')\n",
    "                #torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            else:\n",
    "                if (epoch+1) % 10 == 0:\n",
    "                    #save_path_model = (model_folder + '/model_rgb_state_dict_epoch' + str(epoch+1) + '.pth')\n",
    "                    torch.save(model.state_dict(), save_path_model)\n",
    "    optim_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train temporal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:14:42.693918Z",
     "start_time": "2020-05-24T15:14:42.688965Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 0.01            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-4  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 750      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [150, 300, 500] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.5          # Multiplicative factor for learning rate step-down\n",
    "STACK_SIZE = 5\n",
    "\n",
    "LOG_FREQUENCY = 20\n",
    "\n",
    "model_folder = 'saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:14:43.513826Z",
     "start_time": "2020-05-24T15:14:43.506623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "spatial_transform_train = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
    "                             ToTensor(), normalize])\n",
    "spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:14:48.306884Z",
     "start_time": "2020-05-24T15:14:48.179467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/home/utente/Scaricati/program/ML_DL/FPAR/GTEA61'\n",
    "\n",
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61_flow(DATA_DIR, split='train', transform=spatial_transform_train, seq_len=STACK_SIZE)\n",
    "test_dataset = GTEA61_flow(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=STACK_SIZE)\n",
    "\n",
    "#train_indexes, val_indexes  = train_dataset.split_indices(0.6)\n",
    "\n",
    "#val_dataset = Subset(train_dataset, val_indexes)\n",
    "#train_dataset = Subset(train_dataset, train_indexes)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "#print('Valid Dataset: {}'.format(len(val_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:14:51.068152Z",
     "start_time": "2020-05-24T15:14:51.061653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:15:02.778909Z",
     "start_time": "2020-05-24T15:15:00.048846Z"
    }
   },
   "outputs": [],
   "source": [
    "validate = True\n",
    "\n",
    "model = flow_resnet34(True, channels=2*STACK_SIZE, num_classes=NUM_CLASSES)\n",
    "model.train(True)\n",
    "train_params = list(model.parameters())\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = torch.optim.SGD(train_params, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:01.771262Z",
     "start_time": "2020-05-24T15:16:44.278334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4084,  0.1433,  0.7386,  ..., -0.2264,  0.9150,  0.5325],\n",
      "        [ 1.5826,  1.0154, -1.4570,  ..., -0.7125,  1.0177,  1.7363],\n",
      "        [ 0.7942, -0.3806, -1.0351,  ..., -0.8075,  1.0047,  0.5475],\n",
      "        ...,\n",
      "        [ 1.5178, -1.5933,  0.3368,  ..., -1.9098,  1.4702, -0.5169],\n",
      "        [ 0.1974,  0.8592,  0.0053,  ...,  0.0618, -0.6048,  1.0245],\n",
      "        [ 1.0563, -2.2363,  0.9363,  ..., -1.7047, -0.5733, -1.5339]],\n",
      "       device='cuda:0')\n",
      "Train: Epoch = 1 | Loss = 0.0 | Accuracy = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utente/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0424, -1.5736,  0.6324,  ..., -0.9721, -0.7348,  0.1115],\n",
      "        [ 0.3321, -0.6927, -0.3757,  ...,  0.1693, -0.2389,  0.1666],\n",
      "        [ 3.6670, -1.3592,  0.0680,  ..., -1.3195,  1.4804,  1.5152],\n",
      "        ...,\n",
      "        [-0.5035, -0.5004,  0.7729,  ..., -0.2500,  0.8514, -0.6975],\n",
      "        [ 2.0861,  0.8493, -0.8812,  ...,  0.4804, -0.7253, -0.5652],\n",
      "        [ 0.5314, -1.2240, -1.2776,  ..., -1.0670,  0.3493, -0.3978]],\n",
      "       device='cuda:0')\n",
      "Train: Epoch = 2 | Loss = 0.0 | Accuracy = 0.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/utente/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/utente/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/utente/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/utente/Scaricati/program/ML_DL/FPAR/gtea_dataset.py\", line 145, in __getitem__\n    select_x_frames = frames_x[select_indices]\nIndexError: arrays used as indices must be of integer (or boolean) type\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-06df4cc4b9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mval_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mnumCorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mval_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mval_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/utente/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/utente/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/utente/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/utente/Scaricati/program/ML_DL/FPAR/gtea_dataset.py\", line 145, in __getitem__\n    select_x_frames = frames_x[select_indices]\nIndexError: arrays used as indices must be of integer (or boolean) type\n"
     ]
    }
   ],
   "source": [
    "train_iter = 0\n",
    "min_accuracy = 0\n",
    "cudnn.benchmark\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    trainSamples = 0\n",
    "    iterPerEpoch = 0\n",
    "    model.train(True)\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        iterPerEpoch += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        \n",
    "        trainSamples += inputs.size(0)\n",
    "        output_label, _ = model(inputVariable)\n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        epoch_loss += loss.data.item()\n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch + 1, avg_loss, trainAccuracy))\n",
    "    \n",
    "    if validate:\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            val_iter = 0\n",
    "            val_samples = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                val_samples += inputs.size(0)\n",
    "                inputVariable = inputs.to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_epoch += val_loss.data.item()\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                \n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_iter\n",
    "            print('Validation: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                save_path_model = (model_folder + '/model_flow_state_dict.pth')\n",
    "                #torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            else:\n",
    "                if (epoch+1) % 10 == 0:\n",
    "                    #save_path_model = (model_folder + '/model_flow_state_dict_epoch' + str(epoch+1) + '.pth')\n",
    "                    torch.save(model.state_dict(), save_path_model)\n",
    "    optim_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:20:13.394143Z",
     "start_time": "2020-05-24T13:20:13.390030Z"
    }
   },
   "source": [
    "# 2 Stream joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:01:54.487506Z",
     "start_time": "2020-05-24T15:01:54.483205Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 0.01            # The initial Learning Rate\n",
    "LR_FLOW = 0.0001\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-4  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 250      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = 1 # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.99          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "STACK_SIZE = 5\n",
    "SEQ_LEN = 7\n",
    "\n",
    "LOG_FREQUENCY = 20\n",
    "\n",
    "model_folder = 'saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:01:56.826864Z",
     "start_time": "2020-05-24T15:01:56.821719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "spatial_transform_train = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
    "                             ToTensor(), normalize])\n",
    "spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:01:57.961799Z",
     "start_time": "2020-05-24T15:01:57.894367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/home/utente/Scaricati/program/ML_DL/FPAR/GTEA61'\n",
    "\n",
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61_2Stream(DATA_DIR, split='train', transform=spatial_transform_train, seq_len=SEQ_LEN, \n",
    "                               stack_size=STACK_SIZE)\n",
    "test_dataset = GTEA61_2Stream(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN, \n",
    "                              stack_size=STACK_SIZE)\n",
    "\n",
    "#train_indexes, val_indexes  = train_dataset.split_indices(0.6)\n",
    "\n",
    "#val_dataset = Subset(train_dataset, val_indexes)\n",
    "#train_dataset = Subset(train_dataset, train_indexes)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "#print('Valid Dataset: {}'.format(len(val_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:01:59.751315Z",
     "start_time": "2020-05-24T15:01:59.747225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:48:45.301874Z",
     "start_time": "2020-05-24T14:48:44.873036Z"
    }
   },
   "outputs": [],
   "source": [
    "flowModel = \"best_flow_model.pth\"\n",
    "rgbModel = \"best_rgb_model.pth\"\n",
    "validate = True\n",
    "\n",
    "model = twoStreamAttentionModel(flowModel=flowModel, frameModel=rgbModel, stackSize=STACK_SIZE, memSize=MEM_SIZE,\n",
    "                                    num_classes=NUM_CLASSES)\n",
    "\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "model.train(False)\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.resNet.layer4[0].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.resNet.layer4[0].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "       \n",
    "for params in model.frameModel.resNet.layer4[1].conv1.parameters():\n",
    "    params.requires_grad = True     \n",
    "\n",
    "for params in model.frameModel.resNet.layer4[1].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "       \n",
    "for params in model.frameModel.resNet.layer4[2].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.resNet.layer4[2].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "            \n",
    "for params in model.frameModel.resNet.fc.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.flowModel.layer4.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "\n",
    "model.classifier.train(True)\n",
    "model.flowModel.layer4.train(True)\n",
    "model.frameModel.lstm_cell.train(True)\n",
    "model.frameModel.classifier.train(True)\n",
    "\n",
    "frame_trainable_params = [p for p in model.frameModel.parameters() if p.requires_grad]\n",
    "flow_trainable_params = [p for p in model.flowModel.parameters() if p.requires_grad]\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = torch.optim.SGD([\n",
    "        {'params': frame_trainable_params},\n",
    "        {'params': flow_trainable_params, 'lr': LR_FLOW},\n",
    "    ], lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.StepLR(optimizer_fn, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "min_accuracy = 0\n",
    "cudnn.benchmark\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    iterPerEpoch = 0\n",
    "    model.classifier.train(True)\n",
    "    model.flowModel.layer4.train(True)\n",
    "    model.frameModel.lstm_cell.train(True)\n",
    "    model.frameModel.classifier.train(True)\n",
    "    for j, (inputFlow, inputFrame, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        iterPerEpoch += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        \n",
    "        inputVariableFlow = inputFlow.to(DEVICE)\n",
    "        inputVariableFrame = inputFrame.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        \n",
    "        output_label = model(inputVariableFlow, inputVariableFrame)\n",
    "        loss = loss_fn(F.log_softmax(output_label, dim=1), labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "         _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        epoch_loss += loss.data.item()\n",
    "        \n",
    "    avg_loss = epoch_loss / iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    print('Average training loss after {} epoch = {} '.format(epoch + 1, avg_loss))\n",
    "    print('Training accuracy after {} epoch = {}% '.format(epoch + 1, trainAccuracy))\n",
    "    \n",
    "    if validate:\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            val_iter = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputFlow, inputFrame, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                \n",
    "                inputVariableFlow = inputFlow.to(DEVICE)\n",
    "                inputVariableFrame = inputFrame.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                 \n",
    "                output_label = model(inputVariableFlow, inputVariableFrame)\n",
    "                loss = loss_fn(F.log_softmax(output_label, dim=1), labelVariable)\n",
    "                \n",
    "                val_loss_epoch += val_loss.data.item()\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                    \n",
    "            val_accuracy = (numCorr / valSamples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_iter\n",
    "            print('Val Loss after {} epochs, loss = {}'.format(epoch + 1, avg_val_loss))\n",
    "            print('Val Accuracy after {} epochs = {}%'.format(epoch + 1, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                save_path_model = (model_folder + '/model_twoStream_state_dict.pth')\n",
    "                #torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            else:\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    save_path_model = (model_folder + '/model_twoStream_state_dict_epoch' + str(epoch + 1) + '.pth')\n",
    "                    #torch.save(model.state_dict(), save_path_model)\n",
    "    optim_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
